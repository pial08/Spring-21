{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uRLPr0TnIAHO"
   },
   "outputs": [],
   "source": [
    "BRANCH = 'r1.0.0rc1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "o_0K1lsW1dj9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\\n\\nInstructions for setting up Colab are as follows:\\n1. Open a new Python 3 notebook.\\n2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\\n3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\\n4. Run this cell to set up dependencies.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell\n",
    "\n",
    "# install NeMo\n",
    "#!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dzqD2WDFOIN-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
      "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n"
     ]
    }
   ],
   "source": [
    "from nemo.utils.exp_manager import exp_manager\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "import os\n",
    "import wget \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daYw_Xll2ZR9"
   },
   "source": [
    "# Task Description\n",
    "Given a question and a context both in natural language, predict the span within the context with a start and end position which indicates the answer to the question.\n",
    "For every word in our training dataset we’re going to predict:\n",
    "- likelihood this word is the start of the span \n",
    "- likelihood this word is the end of the span \n",
    "\n",
    "We are using a pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) encoder with 2 span prediction heads for prediction start and end position of the answer. The span predictions are token classifiers consisting of a single linear layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnuziSwJ1yEB"
   },
   "source": [
    "# Dataset\n",
    "This model expects the dataset to be in [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format, e.g. a JSON file for each dataset split. \n",
    "In the following we will show example for a training file. Each title has one or multiple paragraph entries, each consisting of the text - \"context\", and question-answer entries. Each question-answer entry has:\n",
    "* a question\n",
    "* a globally unique id\n",
    "* a boolean flag \"is_impossible\" which shows if the question is answerable or not\n",
    "* in case the question is answerable one answer entry, which contains the text span and its starting character index in the context. If not answerable, the \"answers\" list is empty\n",
    "\n",
    "The evaluation files (for validation and testing) follow the above format except for it can provide more than one answer to the same question. \n",
    "The inference file follows the above format except for it does not require the \"answers\" and \"is_impossible\" keywords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXFORGBv2Jqu"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"Super_Bowl_50\", \n",
    "            \"paragraphs\": [\n",
    "                {\n",
    "                    \"context\": \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\", \n",
    "                    \"qas\": [\n",
    "                        {\n",
    "                            \"question\": \"Where did Super Bowl 50 take place?\", \n",
    "                            \"is_impossible\": \"false\", \n",
    "                            \"id\": \"56be4db0acb8001400a502ee\", \n",
    "                            \"answers\": [\n",
    "                                {\n",
    "                                    \"answer_start\": \"403\", \n",
    "                                    \"text\": \"Santa Clara, California\"\n",
    "                                }\n",
    "                            ]\n",
    "                        },\n",
    "                        {\n",
    "                            \"question\": \"What was the winning score of the Super Bowl 50?\", \n",
    "                            \"is_impossible\": \"true\", \n",
    "                            \"id\": \"56be4db0acb8001400a502ez\", \n",
    "                            \"answers\": [\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL58EWkd2ZVb"
   },
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrbeXhQ_ZlPH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THi6s1Qx2G1k"
   },
   "source": [
    "In this notebook we are going download the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset to showcase how to do training and inference. There are two datasets, SQuAD1.0 and SQuAD2.0. SQuAD 1.1, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles. SQuAD2.0 dataset combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. \n",
    "\n",
    "\n",
    "To download both datasets, we use  [NeMo/examples/nlp/question_answering/get_squad.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/question_answering/get_squad.py). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tv3qXTTR_hBk"
   },
   "outputs": [],
   "source": [
    "# set the following paths\n",
    "DATA_DIR = \"/home/ocistudent/Desktop/Spring-21/NeMoTutorials\"\n",
    "WORK_DIR = \"/home/ocistudent/Desktop/Spring-21/NeMoTutorials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qcz3Djem_hBn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_squad.py already exists\n"
     ]
    }
   ],
   "source": [
    "## download get_squad.py script to download and preprocess the SQuAD data\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "if not os.path.exists(WORK_DIR + '/get_squad.py'):\n",
    "    print('Downloading get_squad.py...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/get_squad.py', WORK_DIR)\n",
    "else:\n",
    "    print ('get_squad.py already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mpzsC41t_hBq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:16:05 get_squad:66] /home/ocistudent/Desktop/Spring-21/NeMoTutorials\r\n",
      "[NeMo I 2021-05-05 13:16:05 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\r\n",
      "[NeMo I 2021-05-05 13:16:05 get_squad:49] ** Download file already exists, skipping download\r\n",
      "[NeMo I 2021-05-05 13:16:05 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\r\n",
      "[NeMo I 2021-05-05 13:16:05 get_squad:49] ** Download file already exists, skipping download\r\n",
      "[NeMo I 2021-05-05 13:16:05 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\r\n",
      "[NeMo I 2021-05-05 13:16:05 get_squad:49] ** Download file already exists, skipping download\r\n",
      "[NeMo I 2021-05-05 13:16:05 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\r\n",
      "[NeMo I 2021-05-05 13:16:05 get_squad:49] ** Download file already exists, skipping download\r\n"
     ]
    }
   ],
   "source": [
    "# download and preprocess the data\n",
    "! python $WORK_DIR/get_squad.py --destDir $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_HLLl6t_hBs"
   },
   "source": [
    "after execution of the above cell, your data folder will contain a subfolder \"squad\" the following 4 files for training and evaluation\n",
    "- v1.1/train-v1.1.json\n",
    "- v1.1/dev-v1.1.json\n",
    "- v2.0/train-v2.0.json\n",
    "- v2.0/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qYHcfxPL_hBt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad:\r\n",
      "v1.1  v2.0\r\n",
      "\r\n",
      "/home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1:\r\n",
      "dev-v1.1.json  train-v1.1.json\r\n",
      "\r\n",
      "/home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v2.0:\r\n",
      "dev-v2.0.json  train-v2.0.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls -LR {DATA_DIR}/squad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdpikZVreLlI"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The input into the model is the concatenation of two tokenized sequences:\n",
    "\" [CLS] query [SEP] context [SEP]\".\n",
    "This is the tokenization used for BERT, i.e. [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) Tokenizer, which uses the [Google's BERT vocabulary](https://github.com/google-research/bert). This tokenizer is configured with `model.tokenizer.tokenizer_name=bert-base-uncased` and is automatically instantiated using [Huggingface](https://huggingface.co/)'s API. \n",
    "The benefit of this tokenizer is that this is compatible with a pretrained BERT model, from which we can finetune instead of training the question answering model from scratch. However, we also support other tokenizers, such as `model.tokenizer.tokenizer_name=sentencepiece`. Unlike the BERT WordPiece tokenizer, the [SentencePiece](https://github.com/google/sentencepiece) tokenizer model needs to be first created from a text file.\n",
    "See [02_NLP_Tokenizers.ipynb](https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/nlp/02_NLP_Tokenizers.ipynb) for more details on how to use NeMo Tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q7Y7nyW_hBv"
   },
   "source": [
    "# Data and Model Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0b0Tn8M_hBv"
   },
   "source": [
    "Note, this is only an example to showcase usage and is not optimized for accuracy. In the following, we will download and adjust the model configuration to create a toy example, where we only use a small fraction of the original dataset. \n",
    "\n",
    "In order to train the full SQuAD model, leave the model parameters from the configuration file unchanged. This sets NUM_SAMPLES=-1 to use the entire dataset, which will slow down performance significantly. We recommend to use bash script and multi-GPU to accelerate this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n8HZrDmr12_-"
   },
   "outputs": [],
   "source": [
    "# This is the model configuration file that we will download, do not change this\n",
    "MODEL_CONFIG = \"question_answering_squad_config.yaml\"\n",
    "\n",
    "# model parameters, play with these\n",
    "BATCH_SIZE = 12\n",
    "MAX_SEQ_LENGTH = 384\n",
    "# specify BERT-like model, you want to use\n",
    "PRETRAINED_BERT_MODEL = \"bert-base-uncased\"\n",
    "TOKENIZER_NAME = \"bert-base-uncased\" # tokenizer name\n",
    "\n",
    "# Number of data examples used for training, validation, test and inference\n",
    "TRAIN_NUM_SAMPLES = VAL_NUM_SAMPLES = TEST_NUM_SAMPLES = 5000 \n",
    "INFER_NUM_SAMPLES = 5\n",
    "\n",
    "TRAIN_FILE = f\"{DATA_DIR}/squad/v1.1/train-v1.1.json\"\n",
    "VAL_FILE = f\"{DATA_DIR}/squad/v1.1/dev-v1.1.json\"\n",
    "TEST_FILE = f\"{DATA_DIR}/squad/v1.1/dev-v1.1.json\"\n",
    "INFER_FILE = f\"{DATA_DIR}/squad/v1.1/dev-v1.1.json\"\n",
    "\n",
    "INFER_PREDICTION_OUTPUT_FILE = \"output_prediction.json\"\n",
    "INFER_NBEST_OUTPUT_FILE = \"output_nbest.json\"\n",
    "\n",
    "# training parameters\n",
    "LEARNING_RATE = 0.00003\n",
    "\n",
    "# number of epochs\n",
    "MAX_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daludzzL2Jba"
   },
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_whKCxfTMo6Y"
   },
   "source": [
    "The model is defined in a config file which declares multiple important sections. They are:\n",
    "- **model**: All arguments that will relate to the Model - language model, span prediction, optimizer and schedulers, datasets and any other related information\n",
    "\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T1gA8PsJ13MJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file is already exists\n"
     ]
    }
   ],
   "source": [
    "# download the model's default configuration file \n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/conf/{MODEL_CONFIG}', config_dir)\n",
    "else:\n",
    "    print ('config file is already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mX3KmWMvSUQw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/Desktop/Spring-21/NeMoTutorials/configs/question_answering_squad_config.yaml\n",
      "name: QA\n",
      "pretrained_model: null\n",
      "do_training: true\n",
      "trainer:\n",
      "  gpus: 1\n",
      "  num_nodes: 1\n",
      "  max_epochs: 2\n",
      "  max_steps: null\n",
      "  accumulate_grad_batches: 1\n",
      "  precision: 16\n",
      "  amp_level: O1\n",
      "  accelerator: ddp\n",
      "  gradient_clip_val: 0.0\n",
      "  val_check_interval: 1.0\n",
      "  checkpoint_callback: false\n",
      "  logger: false\n",
      "  num_sanity_val_steps: 0\n",
      "  log_every_n_steps: 1\n",
      "model:\n",
      "  nemo_path: null\n",
      "  dataset:\n",
      "    version_2_with_negative: false\n",
      "    doc_stride: 128\n",
      "    max_query_length: 64\n",
      "    max_seq_length: 384\n",
      "    max_answer_length: 30\n",
      "    null_score_diff_threshold: 0.0\n",
      "    n_best_size: 20\n",
      "    use_cache: true\n",
      "    do_lower_case: true\n",
      "    num_workers: 2\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "  train_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  validation_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  test_ds:\n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  tokenizer:\n",
      "    tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "    vocab_file: null\n",
      "    tokenizer_model: null\n",
      "    special_tokens: null\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null\n",
      "    config: null\n",
      "  token_classifier:\n",
      "    num_layers: 1\n",
      "    dropout: 0.0\n",
      "    num_classes: 2\n",
      "    activation: relu\n",
      "    log_softmax: false\n",
      "    use_transformer_init: true\n",
      "  optim:\n",
      "    name: adamw\n",
      "    lr: 3.0e-05\n",
      "    weight_decay: 0.0\n",
      "    sched:\n",
      "      name: SquareRootAnnealing\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.0\n",
      "      last_epoch: -1\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: QA\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this line will print the entire default config of the model\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCgWzNBkaQLZ"
   },
   "source": [
    "## Setting up data within the config\n",
    "\n",
    "Among other things, the config file contains dictionaries called dataset, train_ds and validation_ds, test_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
    "\n",
    "Specify data paths using `model.train_ds.file`, `model.valuation_ds.file` and `model.test_ds.file`.\n",
    "\n",
    "Let's now add the data paths to the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LQHCJN-ZaoLp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: QA\n",
      "pretrained_model: null\n",
      "do_training: true\n",
      "trainer:\n",
      "  gpus: 1\n",
      "  num_nodes: 1\n",
      "  max_epochs: 2\n",
      "  max_steps: null\n",
      "  accumulate_grad_batches: 1\n",
      "  precision: 16\n",
      "  amp_level: O1\n",
      "  accelerator: ddp\n",
      "  gradient_clip_val: 0.0\n",
      "  val_check_interval: 1.0\n",
      "  checkpoint_callback: false\n",
      "  logger: false\n",
      "  num_sanity_val_steps: 0\n",
      "  log_every_n_steps: 1\n",
      "model:\n",
      "  nemo_path: null\n",
      "  dataset:\n",
      "    version_2_with_negative: false\n",
      "    doc_stride: 128\n",
      "    max_query_length: 64\n",
      "    max_seq_length: 384\n",
      "    max_answer_length: 30\n",
      "    null_score_diff_threshold: 0.0\n",
      "    n_best_size: 20\n",
      "    use_cache: true\n",
      "    do_lower_case: true\n",
      "    num_workers: 2\n",
      "    pin_memory: false\n",
      "    drop_last: false\n",
      "  train_ds:\n",
      "    file: /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/train-v1.1.json\n",
      "    batch_size: 24\n",
      "    shuffle: true\n",
      "    num_samples: 5000\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  validation_ds:\n",
      "    file: /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/dev-v1.1.json\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: 5000\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  test_ds:\n",
      "    file: /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/dev-v1.1.json\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: 5000\n",
      "    num_workers: ${model.dataset.num_workers}\n",
      "    drop_last: ${model.dataset.drop_last}\n",
      "    pin_memory: ${model.dataset.pin_memory}\n",
      "  tokenizer:\n",
      "    tokenizer_name: bert-base-uncased\n",
      "    vocab_file: null\n",
      "    tokenizer_model: null\n",
      "    special_tokens: null\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null\n",
      "    config: null\n",
      "  token_classifier:\n",
      "    num_layers: 1\n",
      "    dropout: 0.0\n",
      "    num_classes: 2\n",
      "    activation: relu\n",
      "    log_softmax: false\n",
      "    use_transformer_init: true\n",
      "  optim:\n",
      "    name: adamw\n",
      "    lr: 3.0e-05\n",
      "    weight_decay: 0.0\n",
      "    sched:\n",
      "      name: SquareRootAnnealing\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.0\n",
      "      last_epoch: -1\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: QA\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config.model.train_ds.file = TRAIN_FILE\n",
    "config.model.validation_ds.file = VAL_FILE\n",
    "config.model.test_ds.file = TEST_FILE\n",
    "\n",
    "config.model.train_ds.num_samples = TRAIN_NUM_SAMPLES\n",
    "config.model.validation_ds.num_samples = VAL_NUM_SAMPLES\n",
    "config.model.test_ds.num_samples = TEST_NUM_SAMPLES\n",
    "\n",
    "config.model.tokenizer.tokenizer_name = TOKENIZER_NAME\n",
    "\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB96-3sTc3yk"
   },
   "source": [
    "# Building the PyTorch Lightning Trainer\n",
    "\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n",
    "\n",
    "Let's first instantiate a Trainer object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "knF6QeQQdMrH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer config - \n",
      "\n",
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 1\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "precision: 16\n",
      "amp_level: O1\n",
      "accelerator: null\n",
      "gradient_clip_val: 0.0\n",
      "val_check_interval: 1.0\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "num_sanity_val_steps: 0\n",
      "log_every_n_steps: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets modify some trainer configs\n",
    "# checks if we have GPU available and uses it\n",
    "cuda = 1 if torch.cuda.is_available() else 0\n",
    "config.trainer.gpus = cuda\n",
    "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
    "\n",
    "# For mixed precision training, use precision=16 and amp_level=O1\n",
    "\n",
    "config.trainer.max_epochs = MAX_EPOCHS\n",
    "\n",
    "# Remove distributed training flags if only running on a single GPU or CPU\n",
    "config.trainer.accelerator = None\n",
    "\n",
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))\n",
    "\n",
    "trainer = pl.Trainer(**config.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IlEMdVxdr6p"
   },
   "source": [
    "# Setting up a NeMo Experiment¶\n",
    "\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8uztqGAmdrYt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:18:30 exp_manager:208] Experiments will be logged at /home/ocistudent/Desktop/Spring-21/NeMoTutorials/QA/2021-05-05_13-18-30\n",
      "[NeMo I 2021-05-05 13:18:30 exp_manager:548] TensorboardLogger has been set up\n"
     ]
    }
   ],
   "source": [
    "config.exp_manager.exp_dir = WORK_DIR\n",
    "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "\n",
    "# the exp_dir provides a path to the current experiment for easy access\n",
    "exp_dir = str(exp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4jy28fbjekD"
   },
   "source": [
    "# Using an Out-Of-Box Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ins2ZzJckKKo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PretrainedModelInfo(\n",
       " \tpretrained_model_name=qa_squadv1.1_bertbase,\n",
       " \tdescription=Question answering model finetuned from NeMo BERT Base Uncased on SQuAD v1.1 dataset which obtains an exact match (EM) score of 82.78% and an F1 score of 82.78%.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_bertbase/versions/1.0.0rc1/files/qa_squadv1.1_bertbase.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=qa_squadv2.0_bertbase,\n",
       " \tdescription=Question answering model finetuned from NeMo BERT Base Uncased on SQuAD v2.0 dataset which obtains an exact match (EM) score of 75.04% and an F1 score of 78.08%.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv2_0_bertbase/versions/1.0.0rc1/files/qa_squadv2.0_bertbase.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=qa_squadv1_1_bertlarge,\n",
       " \tdescription=Question answering model finetuned from NeMo BERT Large Uncased on SQuAD v1.1 dataset which obtains an exact match (EM) score of 85.44% and an F1 score of 92.06%.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_bertlarge/versions/1.0.0rc1/files/qa_squadv1.1_bertlarge.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=qa_squadv2.0_bertlarge,\n",
       " \tdescription=Question answering model finetuned from NeMo BERT Large Uncased on SQuAD v2.0 dataset which obtains an exact match (EM) score of 80.22% and an F1 score of 83.05%.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv2_0_bertlarge/versions/1.0.0rc1/files/qa_squadv2.0_bertlarge.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=qa_squadv1_1_megatron_cased,\n",
       " \tdescription=Question answering model finetuned from Megatron Cased on SQuAD v1.1 dataset which obtains an exact match (EM) score of 88.18% and an F1 score of 94.07%.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_megatron_cased/versions/1.0.0rc1/files/qa_squadv1.1_megatron_cased.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=qa_squadv2.0_megatron_cased,\n",
       " \tdescription=Question answering model finetuned from Megatron Cased on SQuAD v2.0 dataset which obtains an exact match (EM) score of 84.73% and an F1 score of 87.89%.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv2_0_megatron_cased/versions/1.0.0rc1/files/qa_squadv2.0_megatron_cased.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=qa_squadv1.1_megatron_uncased,\n",
       " \tdescription=Question answering model finetuned from Megatron Unased on SQuAD v1.1 dataset which obtains an exact match (EM) score of 87.61% and an F1 score of 94.00%.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_megatron_uncased/versions/1.0.0rc1/files/qa_squadv1.1_megatron_uncased.nemo\n",
       " ),\n",
       " PretrainedModelInfo(\n",
       " \tpretrained_model_name=qa_squadv2.0_megatron_uncased,\n",
       " \tdescription=Question answering model finetuned from Megatron Uncased on SQuAD v2.0 dataset which obtains an exact match (EM) score of 84.48% and an F1 score of 87.65%.,\n",
       " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv2_0_megatron_uncased/versions/1.0.0rc1/files/qa_squadv2.0_megatron_uncased.nemo\n",
       " )]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list available pretrained models\n",
    "nemo_nlp.models.QAModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iFnzHvkVk-S5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:19:29 cloud:66] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/qa_squadv1_1_bertbase/versions/1.0.0rc1/files/qa_squadv1.1_bertbase.nemo to /home/ocistudent/.cache/torch/NeMo/NeMo_1.0.0rc1/qa_squadv1.1_bertbase/42a5d611ce54ad0e2cc05000e259932f/qa_squadv1.1_bertbase.nemo\n",
      "[NeMo I 2021-05-05 13:24:40 common:654] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-05-05 13:24:43 modelPT:180] Using /tmp/tmp61qo5m_i/tokenizer.vocab_file instead of tokenizer.vocab_file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333b67aaa5be4d968fcbcd5da12ca462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4441474f20574506b155ba2f60fbe1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec399aecb12459581bcbd70c7b9ab8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b2f44e859246b98d662e07bc667eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2021-05-05 13:25:35 modelPT:132] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    file: /datasets/squad/v1.1/train-v1.1.json\n",
      "    batch_size: 3\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 2\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2021-05-05 13:25:35 modelPT:139] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    file: /datasets/squad/v1.1/dev-v1.1.json\n",
      "    batch_size: 3\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 2\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2021-05-05 13:25:35 modelPT:146] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    file: null\n",
      "    batch_size: 24\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 2\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2021-05-05 13:25:35 modelPT:1134] World size can only be set by PyTorch Lightning Trainer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fd44c233b148598f870a7dfc3f82d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo I 2021-05-05 13:28:07 modelPT:376] Model QAModel was successfully restored from /home/ocistudent/.cache/torch/NeMo/NeMo_1.0.0rc1/qa_squadv1.1_bertbase/42a5d611ce54ad0e2cc05000e259932f/qa_squadv1.1_bertbase.nemo.\n"
     ]
    }
   ],
   "source": [
    "# load pretained model\n",
    "pretrained_model_name=\"qa_squadv1.1_bertbase\"\n",
    "model = nemo_nlp.models.QAModel.from_pretrained(model_name=pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FI_nQsJo_11"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tjLhUvL_o7_"
   },
   "source": [
    "Before initializing the model, we might want to modify some of the model configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Xeuc2i7Y_nP5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete list of supported BERT-like models\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RK2xglXyAUOO",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated model config - \n",
      "\n",
      "nemo_path: null\n",
      "dataset:\n",
      "  version_2_with_negative: false\n",
      "  doc_stride: 128\n",
      "  max_query_length: 64\n",
      "  max_seq_length: 384\n",
      "  max_answer_length: 30\n",
      "  null_score_diff_threshold: 0.0\n",
      "  n_best_size: 20\n",
      "  use_cache: true\n",
      "  do_lower_case: true\n",
      "  num_workers: 2\n",
      "  pin_memory: false\n",
      "  drop_last: false\n",
      "train_ds:\n",
      "  file: /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/train-v1.1.json\n",
      "  batch_size: 12\n",
      "  shuffle: true\n",
      "  num_samples: 5000\n",
      "  num_workers: ${model.dataset.num_workers}\n",
      "  drop_last: ${model.dataset.drop_last}\n",
      "  pin_memory: ${model.dataset.pin_memory}\n",
      "validation_ds:\n",
      "  file: /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/dev-v1.1.json\n",
      "  batch_size: 12\n",
      "  shuffle: false\n",
      "  num_samples: 5000\n",
      "  num_workers: ${model.dataset.num_workers}\n",
      "  drop_last: ${model.dataset.drop_last}\n",
      "  pin_memory: ${model.dataset.pin_memory}\n",
      "test_ds:\n",
      "  file: /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/dev-v1.1.json\n",
      "  batch_size: 12\n",
      "  shuffle: false\n",
      "  num_samples: 5000\n",
      "  num_workers: ${model.dataset.num_workers}\n",
      "  drop_last: ${model.dataset.drop_last}\n",
      "  pin_memory: ${model.dataset.pin_memory}\n",
      "tokenizer:\n",
      "  tokenizer_name: bert-base-uncased\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "token_classifier:\n",
      "  num_layers: 1\n",
      "  dropout: 0.0\n",
      "  num_classes: 2\n",
      "  activation: relu\n",
      "  log_softmax: false\n",
      "  use_transformer_init: true\n",
      "optim:\n",
      "  name: adamw\n",
      "  lr: 3.0e-05\n",
      "  weight_decay: 0.0\n",
      "  sched:\n",
      "    name: SquareRootAnnealing\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.0\n",
      "    last_epoch: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add the specified above model parameters to the config\n",
    "config.model.language_model.pretrained_model_name = PRETRAINED_BERT_MODEL\n",
    "config.model.train_ds.batch_size = BATCH_SIZE\n",
    "config.model.validation_ds.batch_size  = BATCH_SIZE\n",
    "config.model.test_ds.batch_size = BATCH_SIZE\n",
    "config.model.optim.lr = LEARNING_RATE\n",
    "\n",
    "print(\"Updated model config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "NgsGLydWo-6-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "100%|██████████| 442/442 [00:18<00:00, 23.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:32:59 qa_dataset:119] loading from /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/train-v1.1.json_cache_train_BertTokenizer_30522_384_128_64_5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 48/48 [00:02<00:00, 21.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:33:02 qa_dataset:119] loading from /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/dev-v1.1.json_cache_eval_BertTokenizer_30522_384_128_64_5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 48/48 [00:02<00:00, 17.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:33:05 qa_dataset:119] loading from /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/dev-v1.1.json_cache_eval_BertTokenizer_30522_384_128_64_5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "# dataset we'll be prepared for training and evaluation during\n",
    "model = nemo_nlp.models.QAModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ592Tx4pzyB"
   },
   "source": [
    "## Monitoring Training Progress\n",
    "Optionally, you can create a Tensorboard visualization to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mTJr16_pp0aS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use tensorboard, please use this notebook in a Google Colab environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google import colab\n",
    "  COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "  COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir {exp_dir}\n",
    "else:\n",
    "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hUvnSpyjp0Dh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:36:28 modelPT:685] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 3e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2021-05-05 13:36:28 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.SquareRootAnnealing object at 0x7fd1a19d9d00>\" \n",
      "    will be used during training (effective maximum steps = 426) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.0\n",
      "    last_epoch: -1\n",
      "    max_steps: 426\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type            | Params\n",
      "-----------------------------------------------\n",
      "0 | bert_model | BertEncoder     | 109 M \n",
      "1 | classifier | TokenClassifier | 1.5 K \n",
      "2 | loss       | SpanningLoss    | 0     \n",
      "-----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "[NeMo W 2021-05-05 13:36:28 nemo_logging:349] /home/ocistudent/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "[NeMo W 2021-05-05 13:36:28 nemo_logging:349] /home/ocistudent/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f9326f94cb4eff81522ecd838b1c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-05-05 13:36:28 nemo_logging:349] /home/ocistudent/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:41:31 qa_model:172] val exact match 60.08\n",
      "[NeMo I 2021-05-05 13:41:31 qa_model:173] val f1 70.8809637707633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 425: val_loss reached 1.66743 (best 1.66743), saving model to \"/home/ocistudent/Desktop/Spring-21/NeMoTutorials/QA/2021-05-05_13-18-30/checkpoints/QA---val_loss=1.67-epoch=0.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start the training\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxBiIKMlH8yv"
   },
   "source": [
    "After training for 1 epoch, exact match on the evaluation data should be around 59.2%, F1 around 70.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynCLBmAWFVsM"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To see how the model performs, let’s run evaluation on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XBMCoXAKFtSd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:02<00:00, 21.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:41:59 qa_dataset:119] loading from /home/ocistudent/Desktop/Spring-21/NeMoTutorials/squad/v1.1/dev-v1.1.json_cache_eval_BertTokenizer_30522_384_128_64_5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeMo W 2021-05-05 13:42:00 nemo_logging:349] /home/ocistudent/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd6ce5d5cb54d8da734276aa83733d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-05 13:43:25 qa_model:172] test exact match 60.08\n",
      "[NeMo I 2021-05-05 13:43:25 qa_model:173] test f1 70.8809637707633\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_exact_match': 60.08,\n",
      " 'test_f1': 70.8809637707633,\n",
      " 'test_loss': tensor(1.6674, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.6674314737319946,\n",
       "  'test_exact_match': 60.08,\n",
       "  'test_f1': 70.8809637707633}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.setup_test_data(test_data_config=config.model.test_ds)\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPdzJVAgSFaJ"
   },
   "source": [
    "# Inference\n",
    "\n",
    "To use the model for creating predictions, let’s run inference on the unlabeled inference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DQhsamclRtxJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:02<00:00, 21.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# # store test prediction under the experiment output folder\n",
    "output_prediction_file = f\"{exp_dir}/{INFER_PREDICTION_OUTPUT_FILE}\"\n",
    "output_nbest_file = f\"{exp_dir}/{INFER_NBEST_OUTPUT_FILE}\"\n",
    "all_preds, all_nbests = model.inference(file=INFER_FILE, batch_size=5, num_samples=INFER_NUM_SAMPLES, output_nbest_file=output_nbest_file, output_prediction_file=output_prediction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sQpRIOaM_hCQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Which NFL team represented the AFC at Super Bowl 50? answer: Denver Broncos\n",
      "question: Which NFL team represented the NFC at Super Bowl 50? answer: Carolina Panthers\n",
      "question: Where did Super Bowl 50 take place? answer: Levi's Stadium\n",
      "question: Which NFL team won Super Bowl 50? answer: Denver Broncos\n",
      "question: What color was used to emphasize the 50th anniversary of the Super Bowl? answer: gold\n",
      "{\r\n",
      "    \"56be4db0acb8001400a502ec\": [\r\n",
      "        \"Which NFL team represented the AFC at Super Bowl 50?\",\r\n",
      "        \"Denver Broncos\"\r\n",
      "    ],\r\n",
      "    \"56be4db0acb8001400a502ed\": [\r\n",
      "        \"Which NFL team represented the NFC at Super Bowl 50?\",\r\n",
      "        \"Carolina Panthers\"\r\n",
      "    ],\r\n",
      "    \"56be4db0acb8001400a502ee\": [\r\n",
      "        \"Where did Super Bowl 50 take place?\",\r\n",
      "        \"Levi's Stadium\"\r\n",
      "    ],\r\n",
      "    \"56be4db0acb8001400a502ef\": [\r\n",
      "        \"Which NFL team won Super Bowl 50?\",\r\n",
      "        \"Denver Broncos\"\r\n",
      "    ],\r\n",
      "    \"56be4db0acb8001400a502f0\": [\r\n",
      "        \"What color was used to emphasize the 50th anniversary of the Super Bowl?\",\r\n",
      "        \"gold\"\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "for _, item in all_preds.items():\n",
    "    print(f\"question: {item[0]} answer: {item[1]}\")\n",
    "#The prediction file contains the predicted answer to each question id for the first TEST_NUM_SAMPLES.\n",
    "! python -m json.tool $exp_dir/$INFER_PREDICTION_OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ref1qSonGNhP"
   },
   "source": [
    "If you have NeMo installed locally, you can also train the model with \n",
    "[NeMo/examples/nlp/question_answering/get_squad.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/question_answering/question_answering_squad.py).\n",
    "\n",
    "To run training script, use:\n",
    "\n",
    "`python question_answering_squad.py model.train_ds.file=TRAIN_FILE model.validation_ds.file=VAL_FILE model.test_ds.file=TEST_FILE`\n",
    "\n",
    "To improve the performance of the model, train with multi-GPU and a global batch size of 24. So if you use 8 GPUs with `trainer.gpus=8`, set `model.train_ds.batch_size=3`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "daYw_Xll2ZR9"
   ],
   "name": "Question_Answering_Squad.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
